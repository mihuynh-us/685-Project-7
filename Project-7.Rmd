---
title: "Project 7"
author: "Ellen Hickman, Mi Huynh"
date: "2025-10-30"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

1. Faraway Chapter 6. Exercise 2.

• Use the teengamb dataset, fit a model with gamble as the response and the other variables as predictors.
```{r}
library(faraway)
data(teengamb)
model <- lm(gamble ~ sex + status + income + verbal, data = teengamb)
summary(model)
# Check the mean, median, variance
summary(resid(model))
```

• Perform regression diagnostics on the model to answer the questions 1.A to 1.G. Display any plots that are relevant. Do not provide plots about which you have nothing to say. Suggest possible improvements or corrections to the model where appropriate.

### 1.A. Check the zero error mean assumption using residual plots. What do you conclude?
```{r}
hist(resid(model))
plot(density(resid(model)), main = "Density Plot")
```
-     Using the histogram and density plot of the residuals, it appears that the 
      residuals are approximately normally distributed around a mean of zero. 
      Therefore, we can conclude that the zero error mean assumption is satisfied.
-     Zero mean assumption means that the average of the residuals is zero. This is a key assumption in linear regression, ensuring that the model's predictions are unbiased on average.
      
### 1.B. Check the constant error variance assumption (homoskedascity) using residual plots as well as a formal testing. What do you conclude?
```{r,fig.width=4, fig.height=4}
plot(fitted(model),resid(model)) # estimated y vs residuals
abline(h=0,col="blue")
```
-   The estimated y vs the residuals plot shows funneling, suggesting that the variance of the residuals is not constant, in other words heteroskedasticity is present. Heteroskedascity means that the variance of the the resdiuals is not the same for each predicted value. Homoskedasticity is constant variance. 
-   Constant variance assumption means that the variance of the residuals is the same across all levels of the independent variables. This is important for the validity of hypothesis tests and confidence intervals (SE) in regression analysis.
-   When constant variance assumption is violated, it can lead to inefficient (need more data to draw the same conclusions) estimates and unreliable (wide CIs) hypothesis tests.
```{r}
cor(fitted(model), resid(model)) 
summary(lm(resid(model)~fitted(model)))
var.test(resid(model)[fitted(model)<=20], # var.test() tests if two variances are equal. Ho = var1/var2 = 1. Ha = var1/var2 != 1
         resid(model)[fitted(model)>20]) #20 aribitrary cutoff to test equal variances across low and high fitted values
```
-   The goal of correlation & predicted model is to show that there is no relation between the predicted y hat (x) and the residuals of the model (y). ####### need help interpreting corr and summary of model. Asking on piazza. ##########
-   Here, our correlation is a large negative number. 
-   Our model 
-   var.test is testing the null hypothesis that the variances of the two groups are equal. The p-value is significant (0.03), so we reject the null hypothesis and conclude that there is a significant difference in variances between the two groups, at the arbitrary cut off of 20 for the fitted model values. 

1.C. Check the error normality assumption. What do you conclude?
```{r,fig.width=4, fig.height=4}
qqnorm(resid(model)) #normal distribution of errors. QQ plot plots the distribution of the residuals against a theoretical normal distribution. If the points fall along a straight line, it suggests that the residuals are normally distributed.
shapiro.test(resid(model)) #null hypothesis is that the data is normally distributed. Ha is that it doesn't follow the normal distribution. fail to reject null
```
-  The QQ plot shows deviation in the tails, suggesting the residuals may not be normally distributed. Additionally, the p-value of the SW test is significant (p > 0.001), having us reject the null that the data is normally distributed.

1.D. Check for large leverage points. Which observations have a large leverage?

1.E. Check for outliers. What do you conclude?

1.F. Check for influential points. What do you conclude?

1.G. Check the structure of the relationship between the predictors and the
response. 